{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_BME_datasets():\n",
    "    train_dataset = arff.loadarff(f'datasets/BME/BME_TRAIN.arff')[0]\n",
    "    test_dataset = arff.loadarff(f'datasets/BME/BME_TEST.arff')[0]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def array_preprocess_BME_dataset(dataset):\n",
    "    serie_length = 64\n",
    "    matrix = np.zeros((dataset.shape[0], serie_length))\n",
    "    labels = np.zeros((dataset.shape[0], 1))\n",
    "\n",
    "    for i, serie in enumerate(dataset):\n",
    "        label = serie[-1]\n",
    "        label = np.frombuffer(label, dtype=np.uint8)[0] - 49\n",
    "\n",
    "        serie = np.array(list(serie)[:-1])[::2]\n",
    "\n",
    "        min_value = np.min(serie)   \n",
    "        max_value = np.max(serie)\n",
    "        serie = (serie - min_value) / (max_value - min_value)\n",
    "        serie = 2*serie - 1\n",
    "            \n",
    "        matrix[i, :] = serie\n",
    "        labels[i] = label\n",
    "        \n",
    "    return matrix.T, labels.flatten().astype('int')\n",
    "\n",
    "train_dataset, test_dataset = load_BME_datasets()\n",
    "train_matrix, train_labels = array_preprocess_BME_dataset(train_dataset)\n",
    "test_matrix, test_labels = array_preprocess_BME_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_matrix, dict_labels = [], []\n",
    "n_test_samples_per_class = 10\n",
    "\n",
    "for c in np.unique(test_labels):\n",
    "    idxs = np.where(test_labels == c)[0]\n",
    "    test_matrix = np.delete(test_matrix, idxs[n_test_samples_per_class:], axis=1)\n",
    "    test_labels = np.delete(test_labels, idxs[n_test_samples_per_class:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = dict()\n",
    "train_D['X'] = train_matrix\n",
    "train_D['labels'] = train_labels\n",
    "\n",
    "with open('datasets/BME/train_D.pickle', 'wb') as f:\n",
    "    pickle.dump(train_D, f)\n",
    "\n",
    "test_D = dict()\n",
    "test_D['X'] = test_matrix\n",
    "test_D['labels'] = test_labels\n",
    "\n",
    "with open('datasets/BME/test_D.pickle', 'wb') as f:\n",
    "    pickle.dump(test_D, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starts_ends(content):\n",
    "    starts, ends = [], []\n",
    "\n",
    "    for i, line in enumerate(content):\n",
    "        if line == '\\n' and i == 1:\n",
    "            continue\n",
    "        elif line == '\\n':\n",
    "            ends.append(i)\n",
    "            starts.append(i+1)\n",
    "\n",
    "    ends = ends[1:]\n",
    "    ends.append(len(content))\n",
    "    return starts, ends\n",
    "\n",
    "def read_digits_line(segment):\n",
    "    label = int(segment[0].split('\"')[1])\n",
    "    X, Y = [], []\n",
    "    for line in segment[3:]:\n",
    "        if not 'PEN' in line and not 'DT' in line:\n",
    "            split_line = line.split(' ')\n",
    "            while '' in split_line:\n",
    "                split_line.remove('')\n",
    "            x = float(split_line[0])\n",
    "            y = float(split_line[1])\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    X = np.array(X)\n",
    "    min_x = np.min(X)\n",
    "    max_x = np.max(X)\n",
    "    X = (X - min_x) / (max_x - min_x)\n",
    "    Y = np.array(Y)\n",
    "    min_y = np.min(Y)\n",
    "    max_y = np.max(Y)\n",
    "    Y = (Y - min_y) / (max_y - min_y)\n",
    "    X = 2*X - 1\n",
    "    Y = 2*Y - 1\n",
    "    return X, Y, label\n",
    "\n",
    "def read_digits_dataset(dataset_type):\n",
    "    dataset_name = 'datasets/digits/pendigits-orig.'\n",
    "    if dataset_type == 'train':\n",
    "        dataset_name += 'tra'\n",
    "    if dataset_type == 'test':\n",
    "        dataset_name += 'tes'\n",
    "    \n",
    "    with open(dataset_name) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    starts, ends = get_starts_ends(content)\n",
    "\n",
    "    all_X, all_Y, all_labels = [], [], []\n",
    "    for start, end in zip(starts, ends):\n",
    "        segment = content[start:end]\n",
    "        X, Y, label = read_digits_line(segment)\n",
    "\n",
    "        all_X.append(X)\n",
    "        all_Y.append(Y)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    return all_X, all_Y, all_labels\n",
    "\n",
    "def load_DIGITS_datasets(dataset_type):\n",
    "    dataset_name = 'datasets/digits/pendigits-orig.'\n",
    "    if dataset_type == 'train':\n",
    "        dataset_name += 'tra'\n",
    "    if dataset_type == 'test':\n",
    "        dataset_name += 'tes'\n",
    "    \n",
    "    with open(dataset_name) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    starts, ends = get_starts_ends(content)\n",
    "\n",
    "    all_X, all_Y, all_labels = [], [], []\n",
    "    for start, end in zip(starts, ends):\n",
    "        segment = content[start:end]\n",
    "        X, Y, label = read_digits_line(segment)\n",
    "\n",
    "        all_X.append(X)\n",
    "        all_Y.append(Y)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    return all_X, all_Y, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(X, Y, labels, max_len, n_samples_per_class):\n",
    "    new_X, new_Y = [], []\n",
    "    new_labels = []\n",
    "    for y in np.unique(labels):\n",
    "        idx_y = np.where(labels == y)[0]\n",
    "        X_y = [X[i] for i in idx_y]\n",
    "        Y_y = [Y[i] for i in idx_y]\n",
    "\n",
    "        minus_lengths = [-len(x) for x in X_y]\n",
    "        idx = np.argsort(minus_lengths)[:n_samples_per_class]\n",
    "        X_y = [X_y[i] for i in idx]\n",
    "        Y_y = [Y_y[i] for i in idx]\n",
    "\n",
    "        new_X.extend(X_y)\n",
    "        new_Y.extend(Y_y)\n",
    "        new_labels.extend([y] * len(X_y))\n",
    "\n",
    "    new_labels = np.array(new_labels)\n",
    "    return new_X, new_Y, new_labels\n",
    "\n",
    "def pad_and_array(X, max_len):\n",
    "    X_array = []\n",
    "    for x in X:\n",
    "        new_x = np.pad(x, (0, max_len - len(x)), 'constant', constant_values=(0, 0))\n",
    "        X_array.append(new_x)\n",
    "    return np.array(X_array)\n",
    "    \n",
    "def array_preprocess_DIGITS_dataset(X, Y, labels, min_len, max_len, n_samples_class_train, n_samples_class_test):\n",
    "    _idx = [i for i, x in enumerate(X) if len(x) <= max_len and len(x) >= min_len]\n",
    "    _X = [X[i] for i in _idx]\n",
    "    _Y = [Y[i] for i in _idx]\n",
    "    labels = np.array([labels[i] for i in _idx])\n",
    "\n",
    "    n_samples = len(_X)\n",
    "\n",
    "    _X_train = _X[::2]\n",
    "    _Y_train = _Y[::2]\n",
    "    train_labels = labels[::2]\n",
    "\n",
    "    _X_test = _X[1::2]\n",
    "    _Y_test = _Y[1::2]\n",
    "    test_labels = labels[1::2]\n",
    "\n",
    "    _X_train, _Y_train, train_labels = crop(_X_train, _Y_train, train_labels, max_len, n_samples_per_class=n_samples_class_train)\n",
    "    _X_train_array = pad_and_array(_X_train, max_len).T\n",
    "    _Y_train_array = pad_and_array(_Y_train, max_len).T\n",
    "\n",
    "    _X_test, _Y_test, test_labels = crop(_X_test, _Y_test, test_labels, max_len, n_samples_per_class=n_samples_class_test)\n",
    "    _X_test_array = pad_and_array(_X_test, max_len).T\n",
    "    _Y_test_array = pad_and_array(_Y_test, max_len).T\n",
    "\n",
    "    return _X_train, _X_train_array, _Y_train, _Y_train_array, train_labels, _X_test, _X_test_array, _Y_test, _Y_test_array, test_labels\n",
    "\n",
    "X_train, Y_train, train_labels = load_DIGITS_datasets('train')\n",
    "\n",
    "train_X, train_matrix_X, train_Y, train_matrix_Y, true_train_labels, test_X, test_matrix_X, test_Y, test_matrix_Y, true_test_labels = array_preprocess_DIGITS_dataset(X_train, Y_train, train_labels, min_len=40, max_len=60, n_samples_class_train=10, n_samples_class_test=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = dict()\n",
    "train_D['X'] = train_X\n",
    "train_D['Y'] = train_Y\n",
    "train_D['matrix_X'] = train_matrix_X\n",
    "train_D['matrix_Y'] = train_matrix_Y\n",
    "train_D['labels'] = true_train_labels\n",
    "\n",
    "with open('datasets/digits/train_D.pickle', 'wb') as f:\n",
    "    pickle.dump(train_D, f)\n",
    "\n",
    "test_D = dict()\n",
    "test_D['X'] = test_X\n",
    "test_D['Y'] = test_Y\n",
    "test_D['matrix_X'] = test_matrix_X\n",
    "test_D['matrix_Y'] = test_matrix_Y\n",
    "test_D['labels'] = true_test_labels\n",
    "\n",
    "with open('datasets/digits/test_D.pickle', 'wb') as f:\n",
    "    pickle.dump(test_D, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
